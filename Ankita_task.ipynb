{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 396 candidates, totalling 1980 fits\n",
      "Model: RandomForestRegressor\n",
      "Best Parameters: {'model__max_depth': 25, 'model__min_samples_leaf': 5, 'model__n_estimators': 20}\n",
      "Mean Absolute Error (MAE): 0.025948368480934233\n",
      "Mean Squared Error (MSE): 0.001035195458975586\n",
      "R-squared (R2): 0.9983714521961964\n",
      "\n",
      "Saved best model to RandomForestRegressor_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration loading and parsing\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def parse_config(config):\n",
    "    design_data = config['design_state_data']\n",
    "    session_info = design_data['session_info']\n",
    "    dataset = session_info['dataset']\n",
    "\n",
    "    target_info = design_data['target']\n",
    "    target = target_info['target']\n",
    "    prediction_type = target_info['prediction_type'].lower()\n",
    "\n",
    "    features_info = design_data['feature_handling']\n",
    "    numerical_features = [\n",
    "        feature for feature, details in features_info.items()\n",
    "        if details['is_selected'] and details['feature_variable_type'] == 'numerical'\n",
    "    ]\n",
    "    categorical_features = [\n",
    "        feature for feature, details in features_info.items()\n",
    "        if details['is_selected'] and details['feature_variable_type'] == 'text'\n",
    "    ]\n",
    "\n",
    "    imputation = {}\n",
    "    for feature, details in features_info.items():\n",
    "        if details['is_selected']:\n",
    "            if 'feature_details' in details and 'missing_values' in details['feature_details']:\n",
    "                if details['feature_details']['impute_with'] == 'Average of values':\n",
    "                    imputation[feature] = 'mean'\n",
    "                elif details['feature_details']['impute_with'] == 'custom':\n",
    "                    imputation[feature] = details['feature_details']['impute_value']\n",
    "                else:\n",
    "                    imputation[feature] = 'median'\n",
    "\n",
    "    feature_generation = design_data.get('feature_generation', {})\n",
    "    feature_reduction = design_data.get('feature_reduction', {})\n",
    "\n",
    "    algorithms = design_data.get('algorithms', {})\n",
    "\n",
    "    return dataset, target, prediction_type, numerical_features, categorical_features, imputation, feature_generation, feature_reduction, algorithms\n",
    "\n",
    "# Dataset\n",
    "def load_data(file_path, features, target):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[features], df[target]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def build_preprocessor(numerical_features, categorical_features, feature_generation, feature_reduction):\n",
    "    numerical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # feature generation\n",
    "    if feature_generation.get('linear_interactions'):\n",
    "        interaction_features = feature_generation['linear_interactions']\n",
    "        for pair in interaction_features:\n",
    "            numerical_pipeline.steps.append(('poly', PolynomialFeatures(degree=2, include_bias=False)))\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if numerical_features:\n",
    "        transformers.append(('num', numerical_pipeline, numerical_features))\n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_pipeline, categorical_features))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    # faeture reduction\n",
    "    if feature_reduction.get('pca', {}).get('is_selected', False):\n",
    "        n_components = feature_reduction['pca'].get('n_components', min(len(numerical_features), 10))\n",
    "        preprocessor = Pipeline([\n",
    "            ('column_transformer', preprocessor),\n",
    "            ('pca', PCA(n_components=n_components))\n",
    "        ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# Model and hyperparameters\n",
    "def get_model_and_params(algorithm_name, algorithm_config):\n",
    "    if not algorithm_config.get('is_selected', False):\n",
    "        return None, None\n",
    "\n",
    "    models = {\n",
    "        'RandomForestRegressor': (RandomForestRegressor(), {\n",
    "            'n_estimators': list(range(algorithm_config.get('min_trees', 10), algorithm_config.get('max_trees', 20) + 1)),\n",
    "            'max_depth': list(range(algorithm_config.get('min_depth', 20), algorithm_config.get('max_depth', 25) + 1)),\n",
    "            'min_samples_leaf': list(range(algorithm_config.get('min_samples_per_leaf_min_value', 5),\n",
    "                                           algorithm_config.get('min_samples_per_leaf_max_value', 10) + 1))\n",
    "        }),\n",
    "        'LinearRegression': (LinearRegression(), {}),\n",
    "        'SVR': (SVR(), {\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'C': [1, 10, 100],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }),\n",
    "        'GradientBoostingRegressor': (GradientBoostingRegressor(), {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }),\n",
    "        'DecisionTreeRegressor': (DecisionTreeRegressor(), {})\n",
    "    }\n",
    "\n",
    "    return models.get(algorithm_name, (None, None))\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred), r2_score(y_test, y_pred)\n",
    "\n",
    "# running main pipeline\n",
    "def run_pipeline(json_file_path):\n",
    "    config = load_config(json_file_path)\n",
    "    dataset, target, prediction_type, numerical_features, categorical_features, imputation, feature_generation, feature_reduction, algorithms = parse_config(config)\n",
    "\n",
    "    X, y = load_data(dataset, numerical_features + categorical_features, target)\n",
    "    preprocessor = build_preprocessor(numerical_features, categorical_features, feature_generation, feature_reduction)\n",
    "    \n",
    "    for algo_name, algo_config in algorithms.items():\n",
    "        model, param_grid = get_model_and_params(algo_name, algo_config)\n",
    "        if model is None:\n",
    "            continue\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        full_param_grid = {f'model__{param}': values for param, values in param_grid.items()}\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        grid_search = GridSearchCV(pipeline, full_param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        mae, mse, r2 = evaluate_model(best_pipeline, X_test, y_test)\n",
    "        \n",
    "        # Print output\n",
    "        print(f\"Model: {algo_name}\")\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "        print(f\"R-squared (R2): {r2}\")\n",
    "        print()\n",
    "        \n",
    "        joblib.dump(best_pipeline, f\"{algo_name}_model.pkl\")\n",
    "        print(f\"Saved best model to {algo_name}_model.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(r'algoparams_from_ui.json')  # json file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
